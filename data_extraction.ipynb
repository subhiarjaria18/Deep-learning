{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMOPCJjl+usigTunSL8bvLh"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import cmudict\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Download NLTK resources (needed for tokenization and cmudict)\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('cmudict')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwIKEOBSBnSb",
        "outputId": "69f4b54f-11b2-4b14-f335-08b3dd78bd3a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Package cmudict is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_article_text(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Extract only article title and text, remove tags and unnecessary parts\n",
        "        title = soup.title.text.strip() if soup.title else \"\"\n",
        "        article_text = \"\"\n",
        "        for paragraph in soup.find_all('p'):\n",
        "            article_text += paragraph.text + \" \"\n",
        "\n",
        "        # Remove extra spaces and newlines\n",
        "        article_text = re.sub(r'\\s+', ' ', article_text).strip()\n",
        "\n",
        "        return title, article_text\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from URL: {url}\")\n",
        "        print(e)\n",
        "        return \"\", \"\"\n"
      ],
      "metadata": {
        "id": "PAKR5gMhF5Is"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load input data\n",
        "input_data = pd.read_excel('Input.xlsx')\n"
      ],
      "metadata": {
        "id": "YFUXl37XF5Lj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an empty DataFrame to store the output\n",
        "output_data = pd.DataFrame(columns=[\n",
        "    'URL_ID', 'URL', 'TITLE', 'ARTICLE_TEXT',\n",
        "    'POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE', 'SUBJECTIVITY SCORE',\n",
        "    'AVG SENTENCE LENGTH', 'PERCENTAGE OF COMPLEX WORDS', 'FOG INDEX',\n",
        "    'AVG NUMBER OF WORDS PER SENTENCE', 'COMPLEX WORD COUNT', 'WORD COUNT',\n",
        "    'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH'\n",
        "])\n"
      ],
      "metadata": {
        "id": "nUpY7bzlF5Os"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Process URLs and Perform Text Analysis (Segmented)"
      ],
      "metadata": {
        "id": "EIzRejBKO1gF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Process each URL\n",
        "for index, row in input_data.iterrows():\n",
        "    url_id = row['URL_ID']\n",
        "    url = row['URL']\n",
        "\n",
        "    # Extract article text from URL\n",
        "    title, article_text = extract_article_text(url)\n",
        "\n",
        "    # Text analysis\n",
        "    blob = TextBlob(article_text)\n",
        "    word_count = len(blob.words)\n",
        "    sentence_count = len(blob.sentences)\n"
      ],
      "metadata": {
        "id": "KHsN0A5XF5Va"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate Syllable Count"
      ],
      "metadata": {
        "id": "bV-folZdO62O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    # Count syllables using cmudict\n",
        "    syllable_count = sum([len(list(cmudict.dict().get(word.lower(), []))) for word in blob.words])\n"
      ],
      "metadata": {
        "id": "A-obRMFoBnVz"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate Complex Word Count and Percentage"
      ],
      "metadata": {
        "id": "dYfkXwKkO8OZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    # Count complex words (words longer than 6 characters)\n",
        "    complex_word_count = len([word for word in blob.words if len(word) > 6])\n",
        "\n",
        "    # Calculate percentage of complex words\n",
        "    percentage_complex_words = (complex_word_count / word_count) * 100 if word_count > 0 else 0\n"
      ],
      "metadata": {
        "id": "zylbp23EHOPf"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate FOG Index"
      ],
      "metadata": {
        "id": "9NC0WRHTPAXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    # Calculate FOG index\n",
        "    avg_sentence_length = word_count / sentence_count if sentence_count > 0 else 0\n",
        "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n"
      ],
      "metadata": {
        "id": "MFh_7n1yHOSc"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Count Personal Pronouns"
      ],
      "metadata": {
        "id": "H13Hx8RDPD1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    # Count personal pronouns (PRP tags)\n",
        "    personal_pronouns = sum(1 for word, pos in blob.tags if pos == 'PRP')\n"
      ],
      "metadata": {
        "id": "YNqc2lcTHOVR"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate Average Word Length and Syllables per Word"
      ],
      "metadata": {
        "id": "u9qqeXAVPGvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    # Calculate average word length\n",
        "    avg_word_length = sum(len(word) for word in blob.words) / word_count if word_count > 0 else 0\n",
        "\n",
        "    # Calculate syllables per word\n",
        "    syllable_per_word = syllable_count / word_count if word_count > 0 else 0\n"
      ],
      "metadata": {
        "id": "iAlgXCodHOYC"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate Polarity and Subjectivity Scores"
      ],
      "metadata": {
        "id": "0TmlaI5uPN-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    # Calculate polarity and subjectivity scores\n",
        "    polarity_score = blob.sentiment.polarity\n",
        "    subjectivity_score = blob.sentiment.subjectivity\n"
      ],
      "metadata": {
        "id": "EH45o_bdHOa_"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save Data to Output DataFrame"
      ],
      "metadata": {
        "id": "sGlQuG70PR1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save data to output DataFrame\n",
        "new_row = {\n",
        "    'URL_ID': url_id,\n",
        "    'URL': url,\n",
        "    'TITLE': title,\n",
        "    'ARTICLE_TEXT': article_text,\n",
        "    'POSITIVE SCORE': polarity_score,\n",
        "    'NEGATIVE SCORE': -polarity_score,  # We can use the negative of polarity as negative score\n",
        "    'POLARITY SCORE': polarity_score,\n",
        "    'SUBJECTIVITY SCORE': subjectivity_score,\n",
        "    'AVG SENTENCE LENGTH': avg_sentence_length,\n",
        "    'PERCENTAGE OF COMPLEX WORDS': percentage_complex_words,\n",
        "    'FOG INDEX': fog_index,\n",
        "    'AVG NUMBER OF WORDS PER SENTENCE': avg_sentence_length,\n",
        "    'COMPLEX WORD COUNT': complex_word_count,\n",
        "    'WORD COUNT': word_count,\n",
        "    'SYLLABLE PER WORD': syllable_per_word,\n",
        "    'PERSONAL PRONOUNS': personal_pronouns,\n",
        "    'AVG WORD LENGTH': avg_word_length\n",
        "}\n",
        "\n",
        "# Convert the new row to a DataFrame\n",
        "new_row_df = pd.DataFrame([new_row])\n",
        "\n",
        "# Concatenate the new row DataFrame with the output DataFrame\n",
        "output_data = pd.concat([output_data, new_row_df], ignore_index=True)\n"
      ],
      "metadata": {
        "id": "pRoYJ8vgHOeb"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save Output to Excel File"
      ],
      "metadata": {
        "id": "gJkI3J8-PVTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save output to Excel file\n",
        "output_data.to_excel('Output.xlsx', index=False)\n"
      ],
      "metadata": {
        "id": "zSFyMnn9BncT"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Approach for Data Extraction and Text Analysis**\n",
        "\n",
        "**Data Extraction:**\n",
        "\n",
        "**Input Data**:\n",
        "* Read the input data from \"Input.xlsx\" containing article URLs.\n",
        "* Each row represents an article with columns like URL_ID, URL, etc.\n",
        "\n",
        "**Data Extraction**:\n",
        "* Use Python libraries like BeautifulSoup, requests, or Scrapy for web scraping.\n",
        "* For each URL in the input data:\n",
        "* Fetch the webpage content.\n",
        "* Use HTML parsing to extract the article title and text.\n",
        "* Avoid extracting header, footer, and other unwanted elements.\n",
        "* Save the extracted article text in a text file with URL_ID as the filename.\n",
        "\n",
        "**Text Analysis**:\n",
        "\n",
        "**Input Data**:\n",
        "\n",
        "* Utilize the extracted article texts for analysis.\n",
        "* Textual Analysis:\n",
        "* Perform the following text analysis tasks for each article:\n",
        "* Tokenization: Split text into sentences and words.\n",
        "\n",
        "**Sentiment Analysis:**\n",
        "\n",
        "* Calculate Positive Score and Negative Score based on predefined positive and negative word lists.\n",
        "* Compute Polarity Score as (Positive Score - Negative Score) / (Positive Score + Negative Score + 1e-9).\n",
        "* Determine Subjectivity Score based on the ratio of Polarity Score to maximum possible Polarity.\n",
        "\n",
        "**Sentence Analysis:**\n",
        "* Calculate Average Sentence Length by dividing the total number of words by the number of sentences.\n",
        "* Identify Complex Words based on predefined criteria.\n",
        "* Compute Percentage of Complex Words as (Complex Word Count / Total Word Count)  100.\n",
        "~ Determine FOG Index as 0.4 * (Average Sentence Length + Percentage of Complex Words).\n",
        "\n",
        "**Word Analysis**:\n",
        "\n",
        "* Calculate Average Number of Words Per Sentence.\n",
        "* Compute Complex Word Count based on predefined criteria.\n",
        "* Determine Word Count.\n",
        "* Calculate Syllables Per Word using syllable count algorithms.\n",
        "* Identify Personal Pronouns (I, me, my, etc.).\n",
        "* Compute Average Word Length.\n",
        "\n",
        "**Output Creation:**\n",
        "\n",
        "* Prepare the output data structure as per the \"Output Data Structure.xlsx\" file.\n",
        "* Each row will represent an article with its URL_ID and extracted variables.\n",
        "* Write the computed variables for each article into the respective columns."
      ],
      "metadata": {
        "id": "HEWix4m-P_0B"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "INFi2vY7Okby"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}